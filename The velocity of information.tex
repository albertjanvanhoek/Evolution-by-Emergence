\documentclass[12pt,a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{geometry}
\geometry{margin=1in}
\usepackage{setspace}
\onehalfspacing

\title{\textbf{The Velocity of Functional Information}}
\author{\textbf{Albert Jan van Hoek}}
\date{January 2026}

\begin{document}

\maketitle

\begin{center}
    \textit{All views and ideas in this paper are my own, developed outside the scope of my employment, and should not be attributed to my employer.}
\end{center}

\vspace{1cm}

\begin{abstract}
The Law of Increasing Functional Information tells us that when configurations are subjected to selection, functional information will accumulate. This observation has been demonstrated across minerals, biological systems, and artificial intelligence. However, these formulations address the direction of change without examining its pace. I argue that while the accumulation of functional information may be inevitable in persistent systems, the velocity of this accumulation varies dramatically and is not strictly tied to the passage of chronological time. By examining what governs this velocity, we shift our attention from determinism to dynamics—from observing that complexity increases to understanding how fast it increases.
\end{abstract}

\section{Time as Medium, Not Driver}
The Law of Increasing Functional Information states that when configurations are subjected to selection, functional information will accumulate. Observing this principle across different domains, there is a temptation to view time as the engine of progress. We speak of "billions of years" for mineral evolution or "millions of generations" for biological evolution.

I contend that time is merely the background medium. The reason reinforcement learning (RL) serves as such an efficient laboratory for observing functional information gain is not simply because it is "fast," but because we have compressed the conditions that drive information accumulation into a human-observable window. A system can persist for eons without meaningful increase in functional information if the conditions for effective selection are weak. Time does not guarantee complexity; something else is at work.

\section{A Framework for Understanding Velocity}
To understand what controls the speed at which functional information accumulates, I propose examining four variables that appear to correlate strongly with evolutionary velocity:

\begin{itemize}
    \item \textbf{Interaction Frequency}: The rate at which a system samples its configuration space. In biological evolution, this is constrained by generation time and population dynamics; in reinforcement learning, by computational cycles. Higher interaction frequency allows a system to explore more combinations per unit of chronological time, increasing the likelihood of discovering rare, functional configurations.
    \item \textbf{Feedback Strength}: The clarity and immediacy with which functional configurations are recognized and retained. Weak feedback—where signals are noisy or selection pressure is mild—means functional discoveries can be lost to entropy before being stabilized. Strong feedback ensures that valuable configurations are quickly incorporated and non-functional ones eliminated.
    \item \textbf{Network Connectivity}: The degree to which different parts of a system can interact and combine. In sparsely connected networks, a functional innovation in one region takes longer to combine with capabilities elsewhere. In highly connected networks, functional patterns can propagate and recombine more rapidly, potentially enabling higher-order complexity to emerge faster.
    \item \textbf{Energy Flux}: The thermodynamic resources available to maintain far-from-equilibrium states and drive variation. Insufficient energy leads systems toward low-complexity equilibrium. Increased energy flux can support more extensive exploration of configuration space, providing more raw material for selection to act upon.
\end{itemize}

These four variables are interconnected but conceptually separable. They offer a framework for analyzing why some evolutionary processes appear explosive while others seem glacially slow.

\section{From Observation to Analysis}
Consider the contrast between mineral evolution and bacterial evolution. Both follow the same underlying principle of functional information accumulation through selection. Yet bacterial populations can undergo substantial evolutionary change in years or months, while comparable complexity gains in mineral systems require geological timescales. The difference is not in the fundamental law but in the values of these four variables.

Reinforcement learning provides an even starker example. When we train an RL agent, we are not discovering a new principle of evolution—we are simply optimizing all four variables simultaneously. We maximize interaction frequency through computational power, provide immediate and precise feedback through reward signals, ensure high connectivity through network architecture, and supply abundant energy. The result is functional information gain that would take biological systems millions of years, compressed into weeks or days.

This suggests that the "pace" of evolution is not a fixed property of the universe but a consequence of specific, identifiable conditions. When these conditions align favorably, we observe what appears to be sudden emergence. When they do not, progress can be imperceptibly slow, regardless of how much chronological time passes.

\section{Implications}
Recognizing that the velocity of functional information gain is variable shifts how we think about evolutionary processes. Rather than viewing progress as an inevitable function of elapsed time, we can analyze the specific conditions that accelerate or decelerate it.

For artificial systems we design—such as AI—this reframes questions of capability development. Instead of asking "when" certain capabilities will emerge, we might ask what values these four variables currently hold and how modifications would affect the pace of development. This provides a more concrete basis for analysis than treating advancement as either inevitable or mysterious.

I believe this framework also illuminates a critical tension in how we organize human society. When we optimize for profit maximization, we often constrain the very variables that drive functional information gain. Proprietary knowledge reduces connectivity. Competition limits interaction frequency between different problem-solvers. Short-term incentives weaken the feedback signals that would favor long-term functional solutions. 

In contrast, when we prioritize open sharing—of knowledge, infrastructure, and innovations—we increase network connectivity and interaction frequency across the entire system. When we strengthen feedback mechanisms that reward genuine improvements to collective wellbeing rather than captured rents, we accelerate the ratchet. Better infrastructure, better healthcare, better solutions to shared problems are not merely desirable outcomes; they are the emergent functions of a society that has organized itself to maximize the velocity of functional information gain. 

\section{Conclusion}
The Law of Increasing Functional Information describes the direction, but examining interaction frequency, feedback strength, network connectivity, and energy flux helps us understand the pace. Reinforcement learning demonstrates that when these variables are optimized, functional information can accumulate in weeks rather than eons—not because we've discovered a shortcut, but because we've made visible the conditions that were always at work.

By shifting our attention from whether complexity increases to how fast it increases, we move from observing determinism to analyzing dynamics. The climb may be inevitable, but understanding what governs its tempo opens new questions about the evolution of complexity itself.

\end{document}